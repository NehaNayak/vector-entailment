\section{Introduction}\label{sec:intro}
  % TODO: Trim and revise
  % Less focus on RNTNs, more on embedding spaces

Natural logic offers a powerful \emph{relational} conception of
semantics: the meanings for expressions are given, at least in part,
by their inferential connections with other expressions
\cite{vanBenthem08NATLOG,maccartney2009extended}. For instance,
\word{turtle} is analyzed, not primarily by its extension in the
world, but rather by its lexical network: it entails \word{reptile},
excludes \word{chair}, is entailed by \word{sea
  turtle}, and so forth. With generalized notions of entailment and
contradiction, these relationships can be defined for all lexical
categories as well as complex phrases, sentences, and even texts. The
resulting theories of meaning offer valuable new analytic tools for
tasks involving database inference, relation extraction, and textual
entailment.

Natural logic aligns well with distributed (e.g., vector)
representations, which also naturally model meaning relationally.
Distributed representations have been used successfully in a wide
array of sophisticated language tasks (e.g., \cite{collobert2011natural}). %,
% including sentiment analysis, analogy completion, relation
% extraction, and named entity recognition.
However, it remains an open question whether it is possible to train
such representations to support the rich, diverse logical reasoning
captured by natural logic; while they excel at synonymy (similarity),
the results are more mixed for entailment, contradiction, and mutual
consistency.  Using the natural logic of \cite{maccartney2009extended}
as our formal model, we address this open question using two neural
network-based models for learning embeddings: plain neural networks
and neural tensor networks (NTNs).  The natural logic is built from
the seven relations defined in Table~\ref{b-table}. Its formal
properties are now well-understood \cite{Icard:Moss:2013:LILT}, so it
provides a rigorous set of goals for our neural models. To keep the
discussion manageable, we limit attention to experiments involving the
lexicon; for a more extended treatment of complex expressions
involving logical connectives and quantifiers, see
\citet{Bowman:Potts:Manning:2014}.

In our experiments, we evaluate these models' ability to learn the
basic algebra of natural logic relations from simulated data and from
the WordNet noun graph. The simulated data help us to achieve analytic
insights into what the models learn, and the WordNet data show how they
fare with a real natural language vocabulary.  We find that only the NTN is able to fully
learn the underlying algebra, but that both models excel in the 
WordNet experiment.

% In our first experiment, we use the pre-specified logical grammar to
% generate controlled data sets and assess the ability of the two
% classes of neural network to learn the core underlying relational
% algebra from this simulated data. In our second experiment, we define
% a simple modification of the logical grammar so that it aligns with
% the WordNet noun graph, and we again assess our two classes of RNN on
% this data. The controlled data of our first experiment helps us to
% achieve analytic insights into what the models are able to learn, and
% the large, diverse lexicon of the second experiment shows us how they
% fare in the real world. 





% These experiments differentiate the increased power of RNTNs better
% than previous work and provide the most convincing demonstration to
% date of the ability of neural networks to model semantic inferences
% in complex natural language sentences.

% TODO: Add citations on related work in structuring relations in embedding spaces:
% - Representing partâ€“whole relations in conceptual spaces
% - Something McCallum/Universal Schema?

\begin{table}[tp]
  \centering\small
  \setlength{\tabcolsep}{15pt}
  \renewcommand{\arraystretch}{1.1}
  \begin{tabular}{l c l l} 
    \toprule
    Name & Symbol & Set-theoretic definition & Example \\ 
    \midrule
    entailment         & $x \natfor y$   & $x \subset y$ & \ii{turtle $\natfor$ reptile}  \\ 
    reverse entailment & $x \natrev y$   & $x \supset y$ & \ii{reptile $\natrev$ turtle}  \\ 
    equivalence        & $x \nateq y$    & $x = y$       & \ii{couch $\nateq$ sofa} \\ 
    alternation        & $x \natalt y$   & $x \cap y = \emptyset \wedge x \cup y \neq \mathcal{D}$ & \ii{turtle $\natalt$ warthog} \\ 
    negation           & $x \natneg y$   & $x \cap y = \emptyset \wedge x \cup y = \mathcal{D}$    & \ii{able $\natneg$ unable} \\
    cover              & $x \natcov y$   & $x \cap y \neq \emptyset \wedge x \cup y = \mathcal{D}$ & \ii{animal $\natcov$ non-turtle} \\ 
    independence       & $x \natind y$   & (else) & \ii{turtle $\natind$ pet}\\
    \bottomrule
  \end{tabular}
  \caption{The seven natural logic relations of \cite{maccartney2009extended}. 
    $\mathcal{D}$ is the universe of possible objects of the same type as those being compared, 
    and the relation $\natind$ applies whenever none of the other six do.} %, including when there 
    %is insufficient knowledge to choose a label.}
  \label{b-table}
\end{table}

